{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://static.zybuluo.com/tc1052400205/sy8rwp8k127v7twfhbufpwp5/H1.webp\n",
      "\n",
      "http://static.zybuluo.com/tc1052400205/8a7lx1ctpoukxp8n523y4itv/F2.webp\n",
      "\n",
      "http://static.zybuluo.com/tc1052400205/dhws20q69him68hvcdc9qp5j/F3.webp\n",
      "\n",
      "http://static.zybuluo.com/tc1052400205/dq4yvhwrlh2aytwh1s6cvw75/F4.webp\n",
      "\n",
      "http://static.zybuluo.com/tc1052400205/ah7bsaojpay5tid2a7oq7yum/F2.png\n",
      "\n",
      "http://static.zybuluo.com/tc1052400205/6xdw3fddcffc8tsvsfxi1aw1/F1.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 需要处理文件名\n",
    "MD_filename = \"自然语言处理.md\"\n",
    "\n",
    "# 新建文件\n",
    "\n",
    "image_dir = \"image/{}\".format(MD_filename.replace(\".md\", \"\"))\n",
    "isExists=os.path.exists(image_dir)\n",
    "# 判断结果\n",
    "if not isExists:\n",
    "    # 如果不存在则创建目录\n",
    "     # 创建目录操作函数\n",
    "    os.makedirs(image_dir)\n",
    "    print(image_dir+' 创建成功')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://static.zybuluo.com/tc1052400205/sy8rwp8k127v7twfhbufpwp5/H1.webp\n",
      "./image/自然语言处理/H1.webp\n",
      "RI\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/H1.webp\n",
      "http://static.zybuluo.com/tc1052400205/8a7lx1ctpoukxp8n523y4itv/F2.webp\n",
      "./image/自然语言处理/F2.webp\n",
      "RI\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F2.webp\n",
      "http://static.zybuluo.com/tc1052400205/dhws20q69him68hvcdc9qp5j/F3.webp\n",
      "./image/自然语言处理/F3.webp\n",
      "RI\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F3.webp\n",
      "http://static.zybuluo.com/tc1052400205/dq4yvhwrlh2aytwh1s6cvw75/F4.webp\n",
      "./image/自然语言处理/F4.webp\n",
      "RI\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F4.webp\n",
      "http://static.zybuluo.com/tc1052400205/ah7bsaojpay5tid2a7oq7yum/F2.png\n",
      "./image/自然语言处理/F2.png\n",
      "�P\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F2.png\n",
      "http://static.zybuluo.com/tc1052400205/6xdw3fddcffc8tsvsfxi1aw1/F1.png\n",
      "./image/自然语言处理/F1.png\n",
      "�P\n",
      "\t https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F1.png\n",
      "#: git branch\n",
      "--->>>\n",
      "\tmain\n",
      "\t* master\n",
      "#: git add -A\n",
      "--->>>\n",
      "#: git commit -m '新增自然语言处理.md文档图片'\n",
      "--->>>\n",
      "\t[master e23ef8c] 新增自然语言处理.md文档图片\n",
      "\t1 file changed, 62 insertions(+), 28 deletions(-)\n",
      "#: git push origin master\n",
      "--->>>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_MD_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-33fcb03ae506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_MD_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_w\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_file_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_MD_filename' is not defined"
     ]
    }
   ],
   "source": [
    "# 替换图片链接\n",
    "new_file_info = []\n",
    "with open(MD_filename) as f_r:\n",
    "    for line in f_r.readlines():\n",
    "        if \"http://static.zybuluo.com/tc1052400205\" in line:\n",
    "            url = line.split(\": \")[1].rstrip()\n",
    "            print(url)\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                img_name = url.split(\"/\")[-1]\n",
    "                image_path = './{}/{}'.format(image_dir, img_name)\n",
    "                print(image_path)\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    new_url = \"https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/{}/{}\".format(image_dir, img_name)\n",
    "                    f.write(response.content) \n",
    "                    print(response.text[0:2])\n",
    "                    line = line.replace(url, new_url)\n",
    "                    print(\"\\t\", new_url)\n",
    "            else:\n",
    "                print(\"下载图片失败\", img_url)\n",
    "        new_file_info.append(line)\n",
    "\n",
    "import os;\n",
    "import sys;\n",
    "\n",
    "def execute_cmd(cmd):\n",
    "    print(\"#:\", cmd)\n",
    "    output=os.popen(cmd);\n",
    "    out=output.readlines();\n",
    "    print(\"--->>>\")\n",
    "    for i in out:\n",
    "        print(\"\\t\" + i.strip());\n",
    "\n",
    "cmd_list = [\n",
    "    \"git branch\",\n",
    "    \"git add -A\",\n",
    "    \"git commit -m '新增{}文档图片'\".format(MD_filename),\n",
    "    \"git push origin master\",\n",
    "]\n",
    "for cmd in cmd_list:\n",
    "    execute_cmd(cmd)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿# 自然语言处理\n",
      "\n",
      "\n",
      "\n",
      "标签（空格分隔）： 深度学习\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "[toc]\n",
      "\n",
      "\n",
      "\n",
      "参考：\n",
      "\n",
      "[NLP系列之文本分类](https://msd.misuland.com/pd/2884249965817761572?page=1)\n",
      "\n",
      "[kaggle之电影文本情感分类](https://blog.csdn.net/lijingpengchina/article/details/52250765)\n",
      "\n",
      "\n",
      "\n",
      "## 基础\n",
      "\n",
      "[LSTM原理和实现](https://www.zybuluo.com/mdeditor#1111865)\n",
      "\n",
      "[RNN原理和实现](https://www.zybuluo.com/mdeditor#1111856)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 1. 预处理，包含文本提取，分词，去停用词，去掉低频词（word2vec也有）等等，\n",
      "\n",
      " 2. 对处理好词数组进行训练词转向量， 如word2vec将词转向量。\n",
      "\n",
      " 3. 对所有出现到词去重，生成每一个词和向量的index。注意处理前需要增加`PAD`和`UNK` 两个词，`PAD`表示的当语句的词长度训练指定不够时需要在前或者在后补，词向量取`np.zeros(self._embeddingSize)`。`UNK`表示分词后不在词典内的词，词向量取`np.random.randn(self._embeddingSize)`。\n",
      "\n",
      " 4. 将标签和句子数值化，句子借助第三步生产的词的index。\n",
      "\n",
      " 5. 初始化训练集和测试集，对数据集进行分割。\n",
      "\n",
      " 6. 构建模型。\n",
      "\n",
      " 7. 进行模型训练。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## 一、预处理\n",
      "\n",
      "### 1.1 文本处理\n",
      "\n",
      "#### 1.1.1 全角转半角\n",
      "\n",
      "[python实现全角半角的相互转换](https://www.cnblogs.com/kaituorensheng/p/3554571.html)\n",
      "\n",
      "```\n",
      "\n",
      "def strQ2B(ustring):\n",
      "\n",
      "    \"\"\"全角转半角\"\"\"\n",
      "\n",
      "    rstring = \"\"\n",
      "\n",
      "    for uchar in ustring:\n",
      "\n",
      "        inside_code = ord(uchar)\n",
      "\n",
      "        if inside_code == 12288: \n",
      "\n",
      "            #全角空格直接转换\n",
      "\n",
      "            inside_code = 32\n",
      "\n",
      "        elif (inside_code >= 65281 and inside_code <= 65374): \n",
      "\n",
      "            #全角字符（除空格）根据关系转化\n",
      "\n",
      "            inside_code -= 65248\n",
      "\n",
      "        rstring += chr(inside_code)\n",
      "\n",
      "    return rstring\n",
      "\n",
      "```\n",
      "\n",
      "#### 1.1.1 编码处理\n",
      "\n",
      "\n",
      "\n",
      "### 1.2 信息提取\n",
      "\n",
      "\n",
      "\n",
      "#### html提取 \n",
      "\n",
      "BeautifulSoup\n",
      "\n",
      "#### 正则\n",
      "\n",
      "\n",
      "\n",
      "### 1.3 分词\n",
      "\n",
      "jieba\n",
      "\n",
      "### 1.4 去停用词\n",
      "\n",
      "NLTK\n",
      "\n",
      "停用词表\n",
      "\n",
      "\n",
      "\n",
      "### 1.5 词频统计\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "from collections import Counter\n",
      "\n",
      "subWords = [\"车辆\",\"车\",\"车主\",\"发动机\",\"维修\"] \n",
      "\n",
      "wordCount = Counter(subWords)  # 统计词频\n",
      "\n",
      "sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
      "\n",
      "\n",
      "\n",
      "# 去除低频词\n",
      "\n",
      "words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
      "\n",
      "```\n",
      "\n",
      "## 二、词处理\n",
      "\n",
      "### 2.1 TF-IDF\n",
      "\n",
      "\n",
      "\n",
      "### 2.2 word2vec\n",
      "\n",
      "\n",
      "\n",
      "参考：[自然语言处理之word2vec](https://www.cnblogs.com/jiangxinyang/p/9332769.html)\n",
      "\n",
      "[词向量详解：从word2vec、glove、ELMo到BERT](https://cpu.baidu.com/api/pc/1022/1329713/detail/36786280402577025/news?cpid=mgxYBQrmp8BJ5gxT4iBoH5Jwe6H_kKt0qLPrvmRdJIMUyRhmFGIOwlcK_B_UvAD7HvknDGWq24MD_OgKZMgWyrtAvzdAPFzQaJcpoIo7O4uBI4GmgV8lrwIS4623uTGZ7nM0X6iqDDT0MIpWdLfCcE32vsUoJ3Zaa70Emvfxx2UhQv_Dzd7191jyEWW6afFpYEGYCd-tNTN_iP6Nd3pMdEtyUrXaI7m828kGADhrS0g_9lKpJ6ApwbfHUg5Tlv57lh1klUEnI2bu2CeMh4Cwny-PHosMlxRedZZHUs_a0W1zQodTTUKZIWT7n9kE-fpYmalGBuMPeqy8euA-ECB6yA&scene=0&no_list=1&forward=api&api_version=2&cds_session_id=b475a40e278146e6988744e4ef34963d&cpu_union_id=IDFA_f1ebbd0c92f2dcb9d122ba8457545c3a&rts=8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "word2vec的中心思想:用一个词附近的其他词来表示该词。\n",
      "\n",
      "\n",
      "\n",
      "word2vec模型其实就是简单化的神经网络，主要包含两种词训练模型：CBOW模型和Skip-gram模型。\n",
      "\n",
      "\n",
      "\n",
      "CBOW模型根据中心词$W(t)$周围的词来预测中心词；Skip-gram模型则根据中心词$W(t)$ 来预测周围的词。\n",
      "\n",
      "\n",
      "\n",
      "1）CBOW模型的第一层是输入层，输入的值是周围每个词的one-hot编码形式，隐藏层只是对输出值做了权值加法，没有激活函数进行非线性的转换，输出值的维度和输入值的维度是一致的。\n",
      "\n",
      "\n",
      "\n",
      "2）Skip-gram模型的第一层是输入层，输入值是中心词的one-hot编码形式，隐藏层只是做线性转换，输出的是输出值的softmax转换后的概率。\n",
      "\n",
      "\n",
      "\n",
      "**词向量的两个优点：**\n",
      "\n",
      "1）降低输入的维度。词向量的维度一般取100-200，对于大样本时的one-hot向量甚至可能达到10000以上。\n",
      "\n",
      "2）增加语义信息。两个语义相近的单词的词向量也是很相似的。\n",
      "\n",
      "\n",
      "\n",
      "#### 2.2.1 CBOW模型\n",
      "\n",
      "参考:\n",
      "\n",
      "[Word2Vec之Skip-Gram与CBOW模型原理](https://www.jianshu.com/p/a2e6a487b385)\n",
      "\n",
      "[基于CBOW训练模型的word2vec](https://blog.csdn.net/weixin_41624658/article/details/82802254)\n",
      "\n",
      "[探秘Word2Vec(四)-CBOW模型](https://www.jianshu.com/p/d534570272a6)\n",
      "\n",
      "\n",
      "\n",
      "CBOW根据上下文预测目标单词，最后使用模型的部分参数作为词向量。\n",
      "\n",
      "训练方式有：基于Huffman树分层Softmax和负采样。 \n",
      "\n",
      "\n",
      "\n",
      "它包括三层，分别为输入层，投影层和输出层，\n",
      "\n",
      "\n",
      "\n",
      "1. 输入层是：\n",
      "\n",
      "包含context(w)中上下文的２×win（窗口）个词向量。即对应目标单词w，选取其上下文各win个单词的词向量作为输入。再所有的向量的求和\n",
      "\n",
      "\n",
      "\n",
      "2. 第二层为投影层：\n",
      "\n",
      "将输入层的２×win个向量做累加求和。\n",
      "\n",
      "\n",
      "\n",
      "3. 输出层: \n",
      "\n",
      "对应一颗二叉树，叶子节点共Ｎ个，对应词典里的每个词(全量的数据构造)。我们是通过哈弗曼树来求得某个词的条件概率的。\n",
      "\n",
      "假设某个词ｗ，从根节点出发到ｗ这个叶子节点，中间会经过４词分支，每一次分支都可以视为一次二分类。从二分类来说，word2ecv定义分到左边为负类（编码为１），分到右边为正类（编码label为０）。在逻辑回归中，一个节点被分为正类的概率为ｐ，分为负类的概率为１－ｐ。将每次分类的结果累乘则得到$p(w∣Context(w))$。概率$p$在逻辑回归二分类问题中，对于任意样本$x=(x1,x2,x3,...,xn)^T$，利用sigmoid函数，求得分为正类的概率为$hθ(w)=σ(θ^Tx)$，负类概率为$1−hθ(w)=σ(θ^Tx)$。\n",
      "\n",
      "\n",
      "\n",
      "词向量是预测的附带产物：\n",
      "\n",
      "\n",
      "\n",
      "1. Huffman树\n",
      "\n",
      "\n",
      "\n",
      "2. Hierarchical Softmax\n",
      "\n",
      "分层Softmax\n",
      "\n",
      "采用了Huffman树构造的条件概率。\n",
      "\n",
      "概率函数：\n",
      "\n",
      "![H1.webp-8.7kB][1]\n",
      "\n",
      "极大似然函数：\n",
      "\n",
      "![F2.webp-7.1kB][2]\n",
      "\n",
      "梯度计算求导，计算出$\\theta$, $\\omega$\n",
      "\n",
      "\n",
      "\n",
      "3. 负采样\n",
      "\n",
      "已知词w的上下文Context(w)，需要预测w，\n",
      "\n",
      "生成它的负采样集合NEG(w)。\n",
      "\n",
      "对于长度为1的线段，词典D中的每一个词根据词频对应线段的一个长度，具体生成词频区间，对词频取了0.75次幂，这个幂实际上是一种“平滑”策略，能够让低频词多一些出场机会，高频词贡献一些出场机会，劫富济贫。\n",
      "\n",
      "然后再随机一个数能不能落在词w分配区间上。计算时是通过查表方式，将上述线段M个“刻度”，刻度之间的间隔是相等的，即1/M。一直按刻度往前进，看该刻度是不是点是不分配的词向量的点。\n",
      "\n",
      "\n",
      "\n",
      "损失函数计算上下文与目标单词之间的点积，采集每一个正样本的同时采集k个负样本。公式的第一项最小化正样本的损失，第二项最大化负样本的损失。现在如果将负样本作为第一项的变量输入，则损失函数结果应该很大。\n",
      "\n",
      "\n",
      "\n",
      "概率函数：\n",
      "\n",
      "![F3.webp-9.2kB][3]\n",
      "\n",
      "极大似然函数：\n",
      "\n",
      "![F4.webp-6.8kB][4]\n",
      "\n",
      "\n",
      "\n",
      "梯度计算求导，计算出$\\theta$, $\\omega$\n",
      "\n",
      "\n",
      "\n",
      "**训练过程：**\n",
      "\n",
      "\n",
      "\n",
      " 1. 准备好语料，将训练数据保存为txt文件中。另取一些数据作为测试数据。\n",
      "\n",
      " 2. 设置一个类class，保存词以及它的哈夫曼树路径、哈弗曼编码、词频\n",
      "\n",
      " 3. 初始化各类参数，扫描语料库，统计词频，并依据每个词的词频生成生成哈弗曼树。生成哈弗曼树后生成每个词的哈弗曼编码以及路径。初始化输入层词向量syn0以及哈弗曼树上非叶子结点的向量syn1。\n",
      "\n",
      " 4. 训练，迭代优化。训练过程中就是通过不断的输入，用随机梯度上升的方法，去更新词向量的值(syn0)和非叶子结点处向量的值(syn1)。实质上就是让词向量在词向量空间中找到正确的位置。\n",
      "\n",
      "训练伪代码如图：\n",
      "\n",
      "\n",
      "\n",
      "#### 2.2.2 gensim使用\n",
      "\n",
      "**word2vec API讲解:**\n",
      "\n",
      "在gensim中，word2vec相关的API都在包gensim.models.word2vec中。和算法有关的参数都在类gensim.models.word2vec.Word2Vec中。算法需要注意的参数有：\n",
      "\n",
      "\n",
      "\n",
      " - sentences：我们要分析的语料，可以是一个列表，或者从文件中遍历读出（word2vec.LineSentence(filename) ）。\n",
      "\n",
      " - size：词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。\n",
      "\n",
      " - window：即词向量上下文最大距离，window越大，则和某一词较远的词也会产生上下文关系。默认值为5，在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5；10]之间。\n",
      "\n",
      " - sg：即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型；是1则是Skip-Gram模型；默认是0即CBOW模型。\n",
      "\n",
      " - hs：即我们的word2vec两个解法的选择了。如果是0，则是Negative Sampling；是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。\n",
      "\n",
      " - negative：即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。\n",
      "\n",
      " - cbow_mean：仅用于CBOW在做投影的时候，为0，则算法中的xw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xw,默认值也是1,不推荐修改默认值。\n",
      "\n",
      " - min_count：需要计算词向量的最小词频。这个值可以去掉一些很生僻的低  - iter：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。\n",
      "\n",
      " - alpha：在随机梯度下降法中迭代的初始步长。算法原理篇中标记为η，默认是0.025。\n",
      "\n",
      " - min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。\n",
      "\n",
      "\n",
      "\n",
      "word2vec是可以进行增量式训练的，因此可以实现一：在输入输入值时可以将数据用生成器的形式导入到模型中；二：可以将数据一个磁盘中读取出来，然后训练完保存模型；之后加载模型再从其他的磁盘上读取数据进行模型的训练。初始化模型的相似度之后，模型就无法再进行增量式训练了，相当于锁定模型了。\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "import time\n",
      "\n",
      "from gensim.models import Word2Vec\n",
      "\n",
      "# 模型参数\n",
      "\n",
      "＃词向量维数\n",
      "\n",
      "num_features = 300  \n",
      "\n",
      "＃最小字数\n",
      "\n",
      "min_word_count = 40 \n",
      "\n",
      "＃并行运行的线程数\n",
      "\n",
      "num_workers = 4 \n",
      "\n",
      "＃上下文窗口大小\n",
      "\n",
      "context = 10                                                        # 常用词的下采样设置                      \n",
      "\n",
      "downsampling = 1e-3 \n",
      "\n",
      "\n",
      "\n",
      "sentences = [[\"a\", \"b\"], [\"c\", \"d   \"]]\n",
      "\n",
      "\n",
      "\n",
      "model = Word2Vec(sentences, workers=num_workers, \\\n",
      "\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "```\n",
      "\n",
      "**最常见的应用：**\n",
      "\n",
      "\n",
      "\n",
      "1. 找出某一个词向量最相近的集合\n",
      "\n",
      "方法：`model.wv.similar_by_word()`\n",
      "\n",
      "从这里可以衍生出去寻找相似的句子，比如“北京下雨了”，可以先进行分词为{“北京”，“下雨了”}，然后找出每个词的前5或者前10个相似的词，\n",
      "\n",
      "比如”北京“的前五个相似词是:{“上海”， “天津\"，”重庆“，”深圳“，”广州“}\n",
      "\n",
      "\"下雨了\"的前五个相似词是:{”下雪了“，”刮风了“，”天晴了“，”阴天了“，”来台风了“}\n",
      "\n",
      "然后将这两个集合随意组合，可以得到25组不同的组合，然后找到这25组中发生概率最大的句子输出。\n",
      "\n",
      "2. 查看两个词向量的相近程度\n",
      "\n",
      "方法：`model.wv.similarity()`\n",
      "\n",
      "比如查看\"北京\"和”上海“之间的相似度\n",
      "\n",
      "3. 找出一组集合中不同的类别\n",
      "\n",
      "方法：`model.wv.doesnt_match()`\n",
      "\n",
      "比如找出集合{“上海”，“天津\"，”重庆“，”深圳“，”北京“}中不同的类别，可能会输出”深圳“，当然也可能输出其他的\n",
      "\n",
      "```\n",
      "\n",
      "import gensim\n",
      "\n",
      "import re\n",
      "\n",
      "import logging\n",
      "\n",
      "import time\n",
      "\n",
      "from gensim.models import word2vec\n",
      "\n",
      "\n",
      "\n",
      "from src.common.utils import dir_and_file_utils\n",
      "\n",
      "\n",
      "\n",
      "# 数据目录\n",
      "\n",
      "data_path = \"/home/kubernetes/code/nlp/sogou/data\"\n",
      "\n",
      "\n",
      "\n",
      "word_embdiing_path = data_path + \"/word_embdiing.txt\"\n",
      "\n",
      "\n",
      "\n",
      "print(word_embdiing_path)\n",
      "\n",
      "sentences = word2vec.LineSentence(word_embdiing_path)\n",
      "\n",
      "print(\"数据行数 = \", list(sentences))\n",
      "\n",
      "\n",
      "\n",
      "# 进行word2vec模型\n",
      "\n",
      "model = gensim.models.Word2Vec(sentences, size=200, sg=1, iter=8, min_count=10, workers=6)  \n",
      "\n",
      "\n",
      "\n",
      "# 持久化模型\n",
      "\n",
      "word2Vec_path = \"./word2Vec.bin\"\n",
      "\n",
      "model.wv.save_word2vec_format(word2Vec_path, binary=True) \n",
      "\n",
      "\n",
      "\n",
      "# 加载word2Vec 模型 \n",
      "\n",
      "model = gensim.models.KeyedVectors.load_word2vec_format(word2Vec_path, binary=True)\n",
      "\n",
      "\n",
      "\n",
      "# 直接取出词向量 \n",
      "\n",
      "vector = model.wv[u'借贷']\n",
      "\n",
      "print(\"词向量 : \", vector)\n",
      "\n",
      "\n",
      "\n",
      "# 计算两个词向量的相似度\n",
      "\n",
      "sim = model.similarity(u'借贷',u'借钱')\n",
      "\n",
      "print(\"相似度 : \", sim)\n",
      "\n",
      "\n",
      "\n",
      "# 获得相近词\n",
      "\n",
      "similar_words = model.similar_by_word(u'采访',topn =5)\n",
      "\n",
      "print(\"相近词 : \", similar_words)\n",
      "\n",
      "\n",
      "\n",
      "# 获得相关词\n",
      "\n",
      "most_similar_words = model.most_similar(u'汽车',topn =5)\n",
      "\n",
      "print(\"相关词 : \", most_similar_words)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 2.3 句子和标签数值化\n",
      "\n",
      "\n",
      "\n",
      "如：句子`[[\"汽车\", \"销售\", \"冠军\"],[\"发动机\", \"维修\"], ,[\"汽车\", \"维修\", \"免费\"]]` 转换为`[[2,3,5],[4,6,0],[2,6,1]]`\n",
      "\n",
      "标签`[\"买\",\"修\",\"修\"]`转换为`[0,1,1]`\n",
      "\n",
      "\n",
      "\n",
      "这里的作用在tensroflow训练传人的数据必须是数值型，所以需要将句子的每一个单词转换为index。还有就是因为在Tensroflow训练时可以直接通过index获取转换为词向量。推理时可以转换为词向量直接推理\n",
      "\n",
      "\n",
      "\n",
      "**生成词的索引**\n",
      "\n",
      "```\n",
      "\n",
      "words = [\"汽车\", \"销售\",\"发动机\", \"冠军\",\"维修\"]\n",
      "\n",
      "\n",
      "\n",
      "# 获取词向量\n",
      "\n",
      "wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
      "\n",
      "\n",
      "\n",
      "# 初始化数组\n",
      "\n",
      "vocab = []\n",
      "\n",
      "wordEmbedding = []\n",
      "\n",
      "\n",
      "\n",
      "# 添加 \"pad\" 和 \"UNK\", \n",
      "\n",
      "vocab.append(\"PAD\")\n",
      "\n",
      "vocab.append(\"UNK\")\n",
      "\n",
      "wordEmbedding.append(np.zeros(self._embeddingSize))\n",
      "\n",
      "wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
      "\n",
      "\n",
      "\n",
      "# 获取词向量并生成数组\n",
      "\n",
      "for word in words:\n",
      "\n",
      "    try:\n",
      "\n",
      "        vector = wordVec.wv[word]\n",
      "\n",
      "        vocab.append(word)\n",
      "\n",
      "        wordEmbedding.append(vector)\n",
      "\n",
      "    except:\n",
      "\n",
      "        print(word + \"不存在于词向量中\")\n",
      "\n",
      "\n",
      "\n",
      "# 词转index\n",
      "\n",
      "word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
      "\n",
      "\n",
      "\n",
      "# 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
      "\n",
      "with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
      "\n",
      "    json.dump(word2idx, f)\n",
      "\n",
      "    \n",
      "\n",
      "# 后续很多地方会用到，如词index转向量\n",
      "\n",
      "wordEmbedding = np.array(wordEmbedding)\n",
      "\n",
      "```\n",
      "\n",
      "**生成去重的标签的索引**\n",
      "\n",
      "```\n",
      "\n",
      "labels = [\"a\", \"b\", \"b\"]\n",
      "\n",
      "uniqueLabel = list(set(labels))\n",
      "\n",
      "label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
      "\n",
      "\n",
      "\n",
      "**将去重的词转索引**       \n",
      "\n",
      "with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
      "\n",
      "    json.dump(label2idx, f)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**将标签和句子数值化**\n",
      "\n",
      "```\n",
      "\n",
      "def _labelToIndex(self, labels, label2idx):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    将标签转换成索引表示\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    labelIds = [label2idx[label] for label in labels]\n",
      "\n",
      "    return labelIds\n",
      "\n",
      "    \n",
      "\n",
      "def _wordToIndex(self, reviews, word2idx):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    将词转换成索引\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
      "\n",
      "    return reviewIds\n",
      "\n",
      "```\n",
      "\n",
      "### 2.4 数值化的句子转向量\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "# 词向量数组，与词的index一致\n",
      "\n",
      "# \"PAD\" 对应的词向量 np.zeros(self._embeddingSize)\n",
      "\n",
      "# \"\"UNK\" 对应的词向量 np.random.randn(self._embeddingSize)\n",
      "\n",
      "wordEmbedding = [......]\n",
      "\n",
      "        \n",
      "\n",
      "# 词嵌入层\n",
      "\n",
      "with tf.name_scope(\"embedding\"):\n",
      "\n",
      "    # 利用预训练的词向量初始化词嵌入矩阵\n",
      "\n",
      "    self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
      "\n",
      "    # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
      "\n",
      "    self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
      "\n",
      "    # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
      "\n",
      "    self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## 三、模型训练\n",
      "\n",
      "\n",
      "\n",
      "### 3.1 数据拆分\n",
      "\n",
      "NLP任务中, 被pad和unk的向量应该赋值为zero还是random呢？\n",
      "\n",
      "比如语义匹配任务，分词后不在词典内的词经常被标为<unk>，处理为相同长度通常会在前或后补<pad>，这两种大家一般选择zero()还是random()来初始化呢？两种方法的区别是什么？\n",
      "\n",
      "```\n",
      "\n",
      "def genTrainEvalData(self, x, y, word2idx, rate):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    x: 词的index [[1, 12, 56, 43], [2, 12, 12]]\n",
      "\n",
      "    y: lable的index [1, 2]\n",
      "\n",
      "    生成训练集和验证集\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    reviews = []\n",
      "\n",
      "    for review in x:\n",
      "\n",
      "        # 如果没有达到设定则补齐\"PAD\"向量\n",
      "\n",
      "        if len(review) >= self._sequenceLength:\n",
      "\n",
      "            reviews.append(review[:self._sequenceLength])\n",
      "\n",
      "        else:\n",
      "\n",
      "            reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
      "\n",
      "   \n",
      "\n",
      "    # 切割点Index     \n",
      "\n",
      "    trainIndex = int(len(x) * rate)\n",
      "\n",
      "    \n",
      "\n",
      "    # 拆分数据集\n",
      "\n",
      "    trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
      "\n",
      "    trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
      "\n",
      "    \n",
      "\n",
      "    evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
      "\n",
      "    evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
      "\n",
      "\n",
      "\n",
      "    return trainReviews, trainLabels, evalReviews, evalLabels\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 3.2 传统机器学习\n",
      "\n",
      "参考：https://blog.csdn.net/u010665216/article/details/78813407\n",
      "\n",
      "\n",
      "\n",
      "传统机器学习方案是\n",
      "\n",
      " \n",
      "\n",
      " 1. 加载word2Vec模型\n",
      "\n",
      " 2. for review in reviews:\n",
      "\n",
      "    1. review 里所有词转向量，np.add(featureVec, model[word])：如review=[\"天气\", \"蓝\"]转为featureVec=[[0.1,-0.2],[0.3,0.4]]\n",
      "\n",
      "    2. 所有词向量进行取平均操作， featureVec = np.divide(featureVec, nwords) 如：[[0.1,-0.2],[0.3,0.4]]变成了[[ 0.05, -0.1 ],[ 0.15, 0.2 ]]\n",
      "\n",
      "    \n",
      "\n",
      "所有传统机器学习模型获取训练数据方案都是一样的：\n",
      "\n",
      "```\n",
      "\n",
      "import logging\n",
      "\n",
      "import gensim\n",
      "\n",
      "from gensim.models import word2vec\n",
      "\n",
      "\n",
      "\n",
      "# 加载word2Vec 模型 \n",
      "\n",
      "word2Vec_path = \"./word2Vec.bin\"\n",
      "\n",
      "model = gensim.models.KeyedVectors.load_word2vec_format(word2Vec_path, binary=True)\n",
      "\n",
      "\n",
      "\n",
      "# 数据目录\n",
      "\n",
      "data_path = \"/home/kubernetes/code/nlp/sogou/data\"\n",
      "\n",
      "\n",
      "\n",
      "# 分词后数据\n",
      "\n",
      "cut_words_path = data_path + \"/cut_words.csv\"\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd \n",
      "\n",
      "df = pd.read_csv(cut_words_path)\n",
      "\n",
      "df.head(10)\n",
      "\n",
      "train_data = [news.split(\" \") for news in df[\"news\"].values]\n",
      "\n",
      "labels = [int(label) for label in df[\"label\"].values]\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def makeFeatureVec(words, model, num_features):\n",
      "\n",
      "    '''\n",
      "\n",
      "    对段落中的所有词向量进行取平均操作\n",
      "\n",
      "    '''\n",
      "\n",
      "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
      "\n",
      "    nwords = 0.\n",
      "\n",
      "\n",
      "\n",
      "    # Index2word包含了词表中的所有词，为了检索速度，保存到set中\n",
      "\n",
      "    index2word_set = set(model.index2word)\n",
      "\n",
      "    for word in words:\n",
      "\n",
      "        if word in index2word_set:\n",
      "\n",
      "            nwords = nwords + 1.\n",
      "\n",
      "            featureVec = np.add(featureVec, model[word])\n",
      "\n",
      "\n",
      "\n",
      "    # 取平均\n",
      "\n",
      "    featureVec = np.divide(featureVec, nwords)\n",
      "\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features):\n",
      "\n",
      "    '''\n",
      "\n",
      "    给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值\n",
      "\n",
      "    '''\n",
      "\n",
      "    counter = 0\n",
      "\n",
      "\n",
      "\n",
      "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
      "\n",
      "\n",
      "\n",
      "    for review in reviews:\n",
      "\n",
      "        if counter % 5000. == 0:\n",
      "\n",
      "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
      "\n",
      "\n",
      "\n",
      "        reviewFeatureVecs[counter, ] = makeFeatureVec(review, model, num_features)\n",
      "\n",
      "\n",
      "\n",
      "        counter = counter + 1\n",
      "\n",
      "    return reviewFeatureVecs\n",
      "\n",
      "\n",
      "\n",
      "num_features = 200\n",
      "\n",
      "%time trainDataVecs = getAvgFeatureVecs(train_data, model, num_features)\n",
      "\n",
      "print(trainDataVecs[0:10])\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "#### 3.2.1 LR\n",
      "\n",
      "```\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "log_reg = LogisticRegression()\n",
      "\n",
      "\n",
      "\n",
      "print(\"逻辑回归分类器10折交叉验证得分: \", np.mean(cross_val_score(log_reg, trainDataVecs, labels, cv=10, scoring='accuracy')))\n",
      "\n",
      "```\n",
      "\n",
      "#### 3.2.2 SVM\n",
      "\n",
      "```\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "# 注意核函数的选择\n",
      "\n",
      "svc=SVC(kernel='poly',degree=2,gamma=1,coef0=0)\n",
      "\n",
      "print(\"SVM分类器10折交叉验证得分: \",\n",
      "\n",
      "      np.mean(cross_val_score(svc, trainDataVecs, labels, cv=4, scoring='accuracy')))\n",
      "\n",
      "```\n",
      "\n",
      "#### 3.2.3 随机森林\n",
      "\n",
      "```\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "forest = RandomForestClassifier(n_estimators = 100, n_jobs=2)\n",
      "\n",
      "\n",
      "\n",
      "print(\"随机森林分类器10折交叉验证得分: \",\n",
      "\n",
      "      np.mean(cross_val_score(forest, trainDataVecs, labels, cv=4, scoring='accuracy')))\n",
      "\n",
      "```\n",
      "\n",
      "#### 3.2.4 贝叶斯\n",
      "\n",
      "```\n",
      "\n",
      "from sklearn.naive_bayes import GaussianNB as GNB\n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "\n",
      "model_GNB = GNB()\n",
      "\n",
      "# model_GNB.fit(trainDataVecs, labels)\n",
      "\n",
      "\n",
      "\n",
      "print(\"高斯贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_GNB, trainDataVecs, labels, cv=10, scoring='accuracy')))\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### 3.3 深度学习\n",
      "\n",
      "#### 3.3.1 textCNN\n",
      "\n",
      "参考：[TextCNN模型原理和实现](https://www.cnblogs.com/bymo/p/9675654.html)\n",
      "\n",
      "\n",
      "\n",
      "优点：能够更好地捕捉局部相关性，类似n-gram\n",
      "\n",
      "\n",
      "\n",
      "将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。如词向量长为128，则卷积核类型为[2,128]，[3,128]，[4,128]，每种类型卷积核有两个，卷积是即各个位置的元素相乘再相加。\n",
      "\n",
      "\n",
      "\n",
      "![F2.png-71.1kB][5]\n",
      "\n",
      "\n",
      "\n",
      "1. Embedding：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。\n",
      "\n",
      "2. Convolution：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel（两个同样大小的卷积核：`Conv1D(filters=2, kernel_size=kernel_size, strides=1)）`。\n",
      "\n",
      "3. MaxPolling：第三层是一个1-MaxPooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。如：\"我觉得这个地方景色还不错，但是人也实在太多了\"，虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用1-max pooling能够很好捕捉这类信息。\n",
      "\n",
      "4. FullConnectionAndSoftmax：最后接一层全连接的softmax层，输出每个类别的概率。\n",
      "\n",
      "\n",
      "\n",
      "**TextCNN模型结构图:**\n",
      "\n",
      "\n",
      "\n",
      "![F1.png-63.8kB][6]\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.2 charCNN\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.3 Bi-LSTM\n",
      "\n",
      "参考：\n",
      "\n",
      "[LSTM](https://www.cnblogs.com/jiangxinyang/p/9362922.html)\n",
      "\n",
      "[BiLSTM](https://www.jianshu.com/p/4999861c26a7)\n",
      "\n",
      "[BiLSTM介绍及代码实现](https://www.jiqizhixin.com/articles/2018-10-24-13)\n",
      "\n",
      "\n",
      "\n",
      "前向的LSTM与后向的LSTM结合成BiLSTM。\n",
      "\n",
      "\n",
      "\n",
      "优点： LSTM更好的捕捉到较长距离的依赖关系，LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息（一些奇怪的句子），通过BiLSTM可以更好的捕捉双向的语义依赖。\n",
      "\n",
      "\n",
      "\n",
      "BiLSTM输出分为两种：横向拼接\n",
      "\n",
      "\n",
      "\n",
      "1. $o^{(t)}=V(h_L^{(t)}+h_R^{(t)})+c$，包含了前向与后向的所有信息，如情感分类任务\n",
      "\n",
      "2. $o^{(t)}=V(h_L^{(t)}+h_R^{(n-t)})+c$，\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.4 Bi-LSTM + Attention\n",
      "\n",
      "\n",
      "\n",
      "优点：Attention是先计算每个时序输出的权重，然后将所有时序的输出进行加权和作为特征向量，然后进行softmax分类。在实验中，加上Attention确实对结果有所提升。\n",
      "\n",
      "\n",
      "\n",
      "Attention 机制：\n",
      "\n",
      "\n",
      "\n",
      "基本思想就是，打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。Attention的实现是通过保留LSTM编码器对输入**蓄力**的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。\n",
      "\n",
      "\n",
      "\n",
      "参考:[易于理解的一些时序相关的操作(LSTM)和注意力机制(Attention Model)](https://blog.csdn.net/wangyanbeilin/article/details/81350683)\n",
      "\n",
      "[LSTM/RNN中的Attention机制](https://www.jianshu.com/p/4b49a1964ddc)\n",
      "\n",
      "[自然语言处理中的Attention机制总结](https://blog.csdn.net/hahajinbu/article/details/81940355)\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.5 RCNN\n",
      "\n",
      "优点：textCNN的基础上使用Bi-LSTM对词向量进行扩充。\n",
      "\n",
      "\n",
      "\n",
      "流程：\n",
      "\n",
      "\n",
      "\n",
      "1. 利用Bi-LSTM获得上下文的信息，类似于语言模型。\n",
      "\n",
      "2. 将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput, wordEmbedding, bwOutput]。\n",
      "\n",
      "3. 将拼接后的向量非线性映射到低维。(降维)\n",
      "\n",
      "4. 向量中的每一个位置的值都取所有时序上的最大值，得到最终的**特征向量**，该过程类似于max-pool。\n",
      "\n",
      "5. softmax分类。\n",
      "\n",
      "\n",
      "\n",
      "举例：　\n",
      "\n",
      " \n",
      "\n",
      " 1. 单词A的向量 = embeddedWords = [0.1, 0.3, 0.3]\n",
      "\n",
      " 2. Bi-LSTM作用下，前向list_m输出list_fw_h=[0.6,0.9]，后向list_m输出list_bw_h=[0.5,0.6], \n",
      "\n",
      " 3. 单词A的新的向量 = [list_fw_h, embeddedWords, list_bw_h] = [0.6, 0.9, 0.1, 0.3, 0.3, 0.5, 0.6]\n",
      "\n",
      " 4. 在对文件进行降维特征分解，选取特征向量较大的值。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.6 Adversarial LSTM\n",
      "\n",
      "Adversarial LSTM的核心思想是通过对wordEmbedding上添加噪音生成对抗样本，将对抗样本以和原始样本同样的形式喂给模型，得到一个AdversarialLoss，通过和原始样本的loss相加得到新的损失，通过优化该新的损失来训练模型，作者认为这种方法能对word embedding加上正则化，避免过拟合。\n",
      "\n",
      "\n",
      "\n",
      "#### 3.3.7 Transformer\n",
      "\n",
      "Transformer：不用进行词训练\n",
      "\n",
      "\n",
      "\n",
      "在Encoder中\n",
      "\n",
      "\n",
      "\n",
      "1. Input 经过嵌入后，要做位置编码，每个输入单词通过词嵌入算法转换为词向量。\n",
      "\n",
      "2. 然后是自注意力层，\n",
      "\n",
      "3. 再经过前馈神经网络层，\n",
      "\n",
      "4. 每个子层之间有残差连接。\n",
      "\n",
      "\n",
      "\n",
      "在Decoder中\n",
      "\n",
      "\n",
      "\n",
      "1. 也有 positional encodings，Multi-head attention 和 FFN，子层之间也要做残差连接，\n",
      "\n",
      "2. 但比 encoder 多了一个 Masked Multi-head attention，\n",
      "\n",
      "3. 最后要经过 Linear 和 softmax 输出概率。\n",
      "\n",
      "\n",
      "\n",
      "每个解码器都可以分解成两个子层。\n",
      "\n",
      "\n",
      "\n",
      " - 自注意力层，从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。如编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。\n",
      "\n",
      " - 前馈神经网络层，自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。\n",
      "\n",
      "\n",
      "\n",
      "**注意：**解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。\n",
      "\n",
      "\n",
      "\n",
      "自注意力层：\n",
      "\n",
      "\n",
      "\n",
      "1. 为编码器的每个输入单词创建三个向量，\n",
      "\n",
      "即查询向量Q、键向量K和值向量V，这些向量在维度上比词嵌入向量更低，如维度是64。\n",
      "\n",
      "2. 计算出Q、K和V。如3个单词[A,B,C]\n",
      "\n",
      "    1. $Q_A = Q * X_A,K_A = K * X_A,V_A = V * X_A, $\n",
      "\n",
      "    1. 对单词A打分，相当于计算相似性，$[Q_A*K_A, Q_A*K_B,Q_A*K_C]$\n",
      "\n",
      "    2. 将计算出的相似性用softmax归一化处理，权重系数。\n",
      "\n",
      "    3. 计算Value值的加权和，利用计算出来的权重系数对各个Value的值加权求和，就得到了我们的Attention的结果。\n",
      "\n",
      "3. 残差\n",
      "\n",
      "$H(x)=F(x)+ x$z\n",
      "\n",
      "4. Add Norm\n",
      "\n",
      "在这里LayerNorm中每个样本都有不同的均值和方差，不像BatchNormalization是整个batch共享均值和方差。\n",
      "\n",
      "多头注意力得到的输入[1,512]和输入x_1[1,512]直接相加[1,512]。再对这一层[x_1,x_2,...,x_n]进行层归一化。\n",
      "\n",
      "\n",
      "\n",
      "多头：\n",
      "\n",
      "多个“表示子空间”（representation subspaces）\n",
      "\n",
      "怎么理解？，\n",
      "\n",
      "我们将其与卷积联系在一起，如果一组Q，K，V相当于一个卷积核，认为不同的卷积核会捕获不同的局部信息，能够提取一个特征[1,64]，那么8组Q，K，V相当于8个卷积核，能够提取八组特征。这个极大的扩展了模型的复杂度，能够学习到更多文本特征。\n",
      "\n",
      "\n",
      "\n",
      "前馈网络层：\n",
      "\n",
      "\n",
      "\n",
      "就是全连接，这边论文中所述是两层。\n",
      "\n",
      "第一层[1,512] * [512,512*4] = [1,2048]\n",
      "\n",
      "Relu\n",
      "\n",
      "第二层[1,2048] * [2048,512] = [1,512]\n",
      "\n",
      "\n",
      "\n",
      "[图解什么是 Transformer](https://www.jianshu.com/p/e7d8caa13b21)\n",
      "\n",
      "[BERT大火却不懂Transformer？读这一篇就够了](https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc)\n",
      "\n",
      "\n",
      "\n",
      "### 3.4 预训练模型\n",
      "\n",
      "#### 3.4.1 ELMo\n",
      "\n",
      "#### 3.4.2 BERT\n",
      "\n",
      "\n",
      "\n",
      "## 四、模型推理\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## 五、优化方案\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  [1]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/H1.webp\n",
      "\n",
      "  [2]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F2.webp\n",
      "\n",
      "  [3]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F3.webp\n",
      "\n",
      "  [4]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F4.webp\n",
      "\n",
      "  [5]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F2.png\n",
      "\n",
      "  [6]: https://raw.githubusercontent.com/youfeng8/aidata_wiki/master/image/自然语言处理/F1.png\n"
     ]
    }
   ],
   "source": [
    "new_MD_filename = \"new\" + MD_filename\n",
    "with open(new_MD_filename, \"w\") as f_w:\n",
    "    f_w.writelines(new_file_info)\n",
    "    for line in new_file_info:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "interline_tag = '\\n<img src=\"https://www.zhihu.com/equation?tex={}\" alt=\"{}\\\\\\\\\" class=\"ee_img tr_noresize\" eeimg=\"1\">\\n'\n",
    "interline_pattern = \"\\$\\$\\n*(.*?)\\n*\\$\\$\"\n",
    "inline_tag = '<img src=\"https://www.zhihu.com/equation?tex={}\" alt=\"{}\" class=\"ee_img tr_noresize\" eeimg=\"1\">'\n",
    "inline_pattern = \"\\$\\n*(.*?)\\n*\\$\"\n",
    "\n",
    "def replace_tex(content):\n",
    "    def dashrepl(matchobj, tag):\n",
    "        formular = matchobj.group(1)\n",
    "        return tag.format(formular, formular)\n",
    "    content = re.sub(interline_pattern, lambda mo: dashrepl(mo, interline_tag), content)\n",
    "    content = re.sub(inline_pattern, lambda mo: dashrepl(mo, inline_tag), content)\n",
    "    return content\n",
    "\n",
    "\n",
    "filename = MD_filename\n",
    "with open(filename, 'r') as f:\n",
    "    content = f.read()\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(replace_tex(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
